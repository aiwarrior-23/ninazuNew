{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878808ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig, XLNetForSequenceClassification\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from pylab import rcParams\n",
    "\n",
    "from torch import nn, optim\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc543cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Number of GPU Available: 1\n",
      "GPU: NVIDIA GeForce GTX 850M\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: {}\".format(torch.cuda.is_available()))\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(\"Number of GPU Available: {}\".format(n_gpu))\n",
    "print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43677680",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('diseaseData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff0b6780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symptoms</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feeling very thirsty. Increased hunger especia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatigue Increased sensitivity to cold Constipa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Symptoms  Disease\n",
       "0  Feeling very thirsty. Increased hunger especia...        1\n",
       "1  Fatigue Increased sensitivity to cold Constipa...        0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "768ec6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a52b9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of class names.\n",
    "class_names = ['Thyroid', 'diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281bb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    return text\n",
    " \n",
    "train['Symptoms'] = train['Symptoms'].apply(clean_text)\n",
    "\n",
    "# Function to convert labels to number.\n",
    "def sentiment2label(sentiment):\n",
    "    if sentiment == \"diabetes\":\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "train['Disease'] = train['Disease'].apply(sentiment2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ad0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class healthDataset(Dataset):\n",
    "\n",
    "    def __init__(self, symptoms, targets, tokenizer, max_len):\n",
    "        self.symptoms = symptoms\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.symptoms)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.symptoms[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = pad_sequences(encoding['input_ids'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "        input_ids = input_ids.astype(dtype = 'int64')\n",
    "        input_ids = torch.tensor(input_ids) \n",
    "\n",
    "        attention_mask = pad_sequences(encoding['attention_mask'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "        attention_mask = attention_mask.astype(dtype = 'int64')\n",
    "        attention_mask = torch.tensor(attention_mask)       \n",
    "\n",
    "        return {\n",
    "        'review_text': review,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "        'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b5fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = healthDataset(\n",
    "        symptoms=train['Symptoms'] .to_numpy(),\n",
    "        targets=train['Disease'] .to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "      )\n",
    "\n",
    "    return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b55f673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "MAX_LEN = 100\n",
    "train_data_loader = create_data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51155bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetForSequenceClassification\n",
    "device = 'cuda'\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6be98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a67ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "  \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].reshape(2,100).to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # preds = preds.cpu().detach().numpy()\n",
    "        _, prediction = torch.max(outputs[1], dim=1)\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        accuracy = metrics.accuracy_score(targets, prediction)\n",
    "\n",
    "        acc += accuracy\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        counter = counter + 1\n",
    "\n",
    "    return acc / counter, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96514672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].reshape(2,100).to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            _, prediction = torch.max(outputs[1], dim=1)\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            prediction = prediction.cpu().detach().numpy()\n",
    "            accuracy = metrics.accuracy_score(targets, prediction)\n",
    "\n",
    "            acc += accuracy\n",
    "            losses.append(loss.item())\n",
    "            counter += 1\n",
    "\n",
    "    return acc / counter, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fccf66bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4414495825767517 Train accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.36280709505081177 Val accuracy 1.0\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6338598728179932 Train accuracy 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.288657546043396 Val accuracy 1.0\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5236546993255615 Train accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.288657546043396 Val accuracy 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,     \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(train)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader, \n",
    "        device, \n",
    "        len(train)\n",
    "    )\n",
    "\n",
    "    print(f'Val loss {val_loss} Val accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), './xlnet_model.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fdfd2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 1.0\n",
      "Test Loss : 0.288657546043396\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  device,\n",
    "  len(train)\n",
    ")\n",
    "\n",
    "print('Test Accuracy :', test_acc)\n",
    "print('Test Loss :', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adf89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "from transformers import XLNetForSequenceClassification\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a068674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
    "model.load_state_dict(torch.load(\"xlnet_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81586e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9035e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import flask\n",
    "from flask import request, jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "app.config[\"DEBUG\"] = True\n",
    "cors = CORS(app)\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "454edfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/anaconda3/envs/Transformers/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"Thyroid\", \"Diabetes\"]\n",
    "\n",
    "@app.route('/api/', methods=['POST'])\n",
    "@cross_origin()\n",
    "def predict_sentiment():\n",
    "    data = request.get_json()\n",
    "    text = str(data[\"data\"])\n",
    "    MAX_LEN = 100\n",
    "    review_text = text\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=False,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = pad_sequences(encoded_review['input_ids'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "    input_ids = input_ids.astype(dtype = 'int64')\n",
    "    input_ids = torch.tensor(input_ids) \n",
    "\n",
    "    attention_mask = pad_sequences(encoded_review['attention_mask'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "    attention_mask = attention_mask.astype(dtype = 'int64')\n",
    "    attention_mask = torch.tensor(attention_mask) \n",
    "\n",
    "    input_ids = input_ids.reshape(1,100).to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    outputs = outputs[0][0].cpu().detach()\n",
    "\n",
    "    probs = F.softmax(outputs, dim=-1).cpu().detach().numpy().tolist()\n",
    "    _, prediction = torch.max(outputs, dim =-1)\n",
    "    \n",
    "    output = {}\n",
    "    output[\"Disease\"] = class_names[prediction]\n",
    "    return jsonify(output)\n",
    "\n",
    "app.run()\n",
    "\n",
    "#     print(\"Positive score:\", probs[1])\n",
    "#     print(\"Negative score:\", probs[0])\n",
    "#     print(f'Review text: {review_text}')\n",
    "#     print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a00b088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive score: 0.46855369210243225\n",
      "Negative score: 0.5314463376998901\n",
      "Review text: Increased sensitivity to cold\n",
      "Sentiment  : Thyroid\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"Increased sensitivity to cold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f59e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/api/', methods=['POST'])\n",
    "@cross_origin()\n",
    "def textTokenizer():\n",
    "    data = request.get_json()\n",
    "    prediction = letsTokenize(str(data[\"data\"]))\n",
    "    return jsonify(prediction)\n",
    "\n",
    "@app.route('/tagger/', methods=['POST'])\n",
    "@cross_origin()\n",
    "def posTagger():\n",
    "    data = request.get_json()\n",
    "    prediction = letsPOSTag(str(data[\"data\"]))\n",
    "    return jsonify(prediction)\n",
    "\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
